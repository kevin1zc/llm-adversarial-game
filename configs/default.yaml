# Default configuration for multi-turn censorship game

model:
  defender:
    model_name: "gpt2"
    max_length: 512
    temperature: 0.7
    use_learned_policy: true
  adversary:
    model_name: "gpt2"
    max_length: 256
    temperature: 0.9
    use_learned_policy: true

game:
  max_turns: 7
  safety_threshold: 0.5
  refuse_threshold: 0.5
  utility_weight: 0.4
  safety_weight: 0.6
  terminate_on_violation: true
  benign_prob: 0.5
  lambda_violation: 5.0
  mu_refusal: 0.2
  gamma_turn: 0.01

training:
  mode: "equilibrium"
  num_episodes: 1000
  batch_size: 32
  defender_lr: 0.00001
  adversary_lr: 0.00001
  gamma: 0.99
  eval_interval: 200
  eval_num_malicious: 100
  eval_num_benign: 100
  checkpoint_dir: "checkpoints"
  update_mode: "alternating"
  warmstart_steps: 200

data:
  train_datasets:
    - "xguard"
  val_datasets:
    - "mhj"
  data_dir: "data"
  split_seed: 42

logging:
  use_wandb: false
  log_dir: "logs"
  log_interval: 20
